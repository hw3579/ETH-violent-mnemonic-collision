import random
import time
import os
import sys
import argparse
import hashlib
import json
from mnemonic import Mnemonic
from web3 import Web3
from eth_account import Account
from colorthon import Colors
import threading
import queue
import signal
import asyncio
import aiohttp
import concurrent.futures

# æ·»åŠ å‘½ä»¤è¡Œå‚æ•°è§£æ
parser = argparse.ArgumentParser(description='ETH Mnemonic Collision Finder - å¼‚æ­¥æ‰¹é‡æŸ¥è¯¢ç‰ˆ')
parser.add_argument('--shard', type=int, default=0, help='å½“å‰æœåŠ¡å™¨åˆ†ç‰‡ID (0-based)')
parser.add_argument('--total-shards', type=int, default=1, help='æ€»åˆ†ç‰‡æ•°é‡')
parser.add_argument('--gen-threads', type=int, default=4, help='ç”ŸæˆåŠ©è®°è¯çš„çº¿ç¨‹æ•°é‡')
parser.add_argument('--query-concurrency', type=int, default=10, help='æŸ¥è¯¢å¹¶å‘æ•°')
parser.add_argument('--resume', action='store_true', help='æ˜¯å¦ä»ä¸Šæ¬¡ä¿å­˜çš„è¿›åº¦ç»§ç»­')
parser.add_argument('--queue-size', type=int, default=10000, help='å¾…æŸ¥è¯¢é˜Ÿåˆ—å¤§å°')
parser.add_argument('--batch-size', type=int, default=50, help='æ¯æ‰¹æŸ¥è¯¢çš„åœ°å€æ•°é‡')
parser.add_argument('--seed', type=int, help='éšæœºç§å­ï¼Œç”¨äºç¡®ä¿é‡ç°æ€§')
args = parser.parse_args()

# å¦‚æœæä¾›äº†ç§å­å‚æ•°ï¼Œè®¾ç½®éšæœºç§å­
if args.seed:
    random.seed(args.seed)

Account.enable_unaudited_hdwallet_features()

# å…¨å±€è®¡æ•°å™¨ï¼Œä½¿ç”¨é”ä¿æŠ¤å¹¶å‘è®¿é—®
counter_lock = threading.Lock()
z = 0  # æ€»å°è¯•æ¬¡æ•°
ff = 0  # æ‰¾åˆ°çš„æœ‰ä½™é¢é’±åŒ…æ•°

# æ‰“å°é”ï¼Œé¿å…è¾“å‡ºæ··ä¹±
print_lock = threading.Lock()

# æ·»åŠ åœæ­¢äº‹ä»¶ï¼Œç”¨äºé€šçŸ¥çº¿ç¨‹é€€å‡º
stop_event = threading.Event()

# é¢œè‰²å®šä¹‰
red = Colors.RED
green = Colors.GREEN
cyan = Colors.CYAN
yellow = Colors.YELLOW
reset = Colors.RESET

# åˆ›å»ºåˆ†ç‰‡è¿›åº¦ç›®å½•
progress_dir = "progress"
if not os.path.exists(progress_dir):
    os.makedirs(progress_dir)

# åˆ†ç‰‡ç›¸å…³å˜é‡
shard_id = args.shard
total_shards = args.total_shards
progress_file = f"{progress_dir}/progress_shard_{shard_id}.txt"

# æ¯ä¸ªåˆ†ç‰‡ä½¿ç”¨ç‹¬ç«‹çš„ç»“æœæ–‡ä»¶
result_file = f"88_shard_{shard_id}.txt"

# åœ°å€ç”Ÿæˆå’ŒæŸ¥è¯¢é˜Ÿåˆ—
address_queue = queue.Queue(maxsize=args.queue_size)

# å®šä¹‰ä¿¡å·å¤„ç†å‡½æ•°
def handle_interrupt(signum, frame):
    print(f"\n{yellow}æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢æ‰€æœ‰çº¿ç¨‹...{reset}")
    stop_event.set()  # è®¾ç½®åœæ­¢äº‹ä»¶é€šçŸ¥æ‰€æœ‰çº¿ç¨‹

# æ³¨å†Œä¿¡å·å¤„ç†å‡½æ•°
signal.signal(signal.SIGINT, handle_interrupt)

# å¼‚æ­¥æŸ¥è¯¢å¾ªç¯å¯¹è±¡
query_loop = None

# RPCèŠ‚ç‚¹æ± ç±» - å¼‚æ­¥ç‰ˆæœ¬
class AsyncRPCNodePool:
    def __init__(self, rpc_file, batch_info_file=None):
        self.nodes = []
        self.node_status = {}  # è®°å½•èŠ‚ç‚¹çŠ¶æ€å’Œå¤±è´¥æ¬¡æ•°
        self.batch_info = {}   # è®°å½•èŠ‚ç‚¹æ‰¹é‡æŸ¥è¯¢èƒ½åŠ›
        self.pool_lock = threading.Lock()
        self.load_nodes(rpc_file)
        self.load_batch_info(batch_info_file)
        
    def load_nodes(self, rpc_file):
        try:
            with open(rpc_file, "r", encoding="utf-8") as file:
                self.nodes = [url.strip() for url in file.readlines() if url.strip()]
            
            for node in self.nodes:
                self.node_status[node] = {
                    "fails": 0,
                    "last_used": 0,
                    "available": True
                }
                
            print(f"{green}å·²åŠ è½½ {len(self.nodes)} ä¸ªRPCèŠ‚ç‚¹{reset}")
        except Exception as e:
            print(f"{red}æ— æ³•åŠ è½½RPCèŠ‚ç‚¹: {e}{reset}")
            sys.exit(1)
    
    def load_batch_info(self, batch_info_file):
        # é»˜è®¤å€¼ - æ‰€æœ‰èŠ‚ç‚¹é»˜è®¤æ”¯æŒå•æ¬¡æŸ¥è¯¢
        for node in self.nodes:
            self.batch_info[node] = {
                "batch_support": False,
                "max_batch_size": 1,
                "speedup": 1.0
            }
        
        # å¦‚æœæä¾›äº†æ‰¹é‡ä¿¡æ¯æ–‡ä»¶ï¼Œä»ä¸­åŠ è½½
        if batch_info_file and os.path.exists(batch_info_file):
            try:
                with open(batch_info_file, "r") as f:
                    data = json.load(f)
                
                for item in data:
                    url = item.get("url")
                    if url and url in self.nodes:
                        self.batch_info[url] = {
                            "batch_support": item.get("batch_request", False) or (item.get("speedup", 0) > 1.5),
                            "max_batch_size": item.get("max_batch_size", 50) if item.get("batch_request", False) else 1,
                            "speedup": item.get("speedup", 1.0)
                        }
                
                print(f"{green}å·²åŠ è½½ {len(data)} ä¸ªèŠ‚ç‚¹çš„æ‰¹é‡æŸ¥è¯¢ä¿¡æ¯{reset}")
                
                # æ‰“å°æ”¯æŒæ‰¹é‡æŸ¥è¯¢çš„èŠ‚ç‚¹ä¿¡æ¯
                batch_nodes = [node for node in self.nodes if self.batch_info[node]["batch_support"]]
                print(f"{green}æ”¯æŒæ‰¹é‡æŸ¥è¯¢çš„èŠ‚ç‚¹: {len(batch_nodes)}/{len(self.nodes)}{reset}")
                for node in batch_nodes[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    max_size = self.batch_info[node]["max_batch_size"]
                    speedup = self.batch_info[node]["speedup"]
                    print(f"{cyan}èŠ‚ç‚¹ {node} æ”¯æŒæœ€å¤§æ‰¹é‡ {max_size}, åŠ é€Ÿæ¯” {speedup:.2f}x{reset}")
                if len(batch_nodes) > 5:
                    print(f"{cyan}...ç­‰å…± {len(batch_nodes)} ä¸ªèŠ‚ç‚¹æ”¯æŒæ‰¹é‡æŸ¥è¯¢{reset}")
                    
            except Exception as e:
                print(f"{yellow}åŠ è½½æ‰¹é‡æŸ¥è¯¢ä¿¡æ¯å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤è®¾ç½®{reset}")
    
    async def get_best_batch_node(self):
        """è·å–å½“å‰æœ€ä½³å¯ç”¨çš„æ‰¹é‡æŸ¥è¯¢èŠ‚ç‚¹"""
        with self.pool_lock:
            # é¦–å…ˆå°è¯•æ‰¾åˆ°æ”¯æŒæ‰¹é‡æŸ¥è¯¢çš„å¯ç”¨èŠ‚ç‚¹
            batch_nodes = [(node, info) for node, info in self.batch_info.items() 
                          if info["batch_support"] and self.node_status[node]["available"]]
            
            # æŒ‰æ‰¹é‡å¤§å°å’ŒåŠ é€Ÿæ¯”æ’åº
            batch_nodes.sort(key=lambda x: (x[1]["max_batch_size"], x[1]["speedup"]), reverse=True)
            
            # å¦‚æœæœ‰æ”¯æŒæ‰¹é‡çš„èŠ‚ç‚¹ï¼Œè¿”å›æœ€ä½³çš„
            if batch_nodes:
                node, info = batch_nodes[0]
                self.node_status[node]["last_used"] = time.time()
                return node, info["max_batch_size"]
            
            # å¦‚æœæ²¡æœ‰æ‰¹é‡èŠ‚ç‚¹å¯ç”¨ï¼Œè¿”å›ä»»æ„å¯ç”¨èŠ‚ç‚¹
            for node in self.nodes:
                if self.node_status[node]["available"]:
                    self.node_status[node]["last_used"] = time.time()
                    return node, 1
            
            # å¦‚æœæ²¡æœ‰å¯ç”¨èŠ‚ç‚¹ï¼Œé‡ç½®æ‰€æœ‰èŠ‚ç‚¹å¹¶è¿”å›ç¬¬ä¸€ä¸ª
            for node in self.nodes:
                self.node_status[node]["fails"] = 0
                self.node_status[node]["available"] = True
            
            return self.nodes[0] if self.nodes else None, 1
    
    def mark_node_fail(self, node):
        """æ ‡è®°èŠ‚ç‚¹å¤±è´¥"""
        with self.pool_lock:
            if node in self.node_status:
                self.node_status[node]["fails"] += 1
                
                # å¦‚æœè¿ç»­å¤±è´¥è¶…è¿‡é˜ˆå€¼ï¼Œæš‚æ—¶æ ‡è®°ä¸ºä¸å¯ç”¨
                if self.node_status[node]["fails"] >= 3:
                    self.node_status[node]["available"] = False
                    print(f"{yellow}RPCèŠ‚ç‚¹æš‚æ—¶ä¸å¯ç”¨: {node}{reset}")
                    
                    # 60ç§’åè‡ªåŠ¨æ¢å¤
                    threading.Timer(60, self.reset_node_status, args=[node]).start()
    
    def mark_node_success(self, node):
        """æ ‡è®°èŠ‚ç‚¹æˆåŠŸ"""
        with self.pool_lock:
            if node in self.node_status:
                self.node_status[node]["fails"] = 0
    
    def reset_node_status(self, node):
        """é‡ç½®èŠ‚ç‚¹çŠ¶æ€"""
        with self.pool_lock:
            if node in self.node_status:
                self.node_status[node]["fails"] = 0
                self.node_status[node]["available"] = True
                print(f"{green}RPCèŠ‚ç‚¹å·²æ¢å¤å¯ç”¨: {node}{reset}")

# åˆå§‹åŒ–RPCèŠ‚ç‚¹æ± 
rpc_pool = AsyncRPCNodePool("rpc.txt", "rpc_test_results/all_rpc_results.json")

# å¼‚æ­¥æ‰¹é‡æ£€æŸ¥åœ°å€ä½™é¢
async def async_batch_check_balances(session, addresses, batch_id):
    """ä½¿ç”¨aiohttpå¼‚æ­¥æ‰¹é‡æ£€æŸ¥åœ°å€ä½™é¢"""
    rpc_url, batch_size = await rpc_pool.get_best_batch_node()
    
    if not rpc_url:
        print(f"{red}æ‰¹æ¬¡ {batch_id} æ²¡æœ‰å¯ç”¨çš„RPCèŠ‚ç‚¹{reset}")
        return None, addresses
    
    # é™åˆ¶æ‰¹é‡å¤§å°ä¸è¶…è¿‡èŠ‚ç‚¹æ”¯æŒçš„æœ€å¤§å€¼
    batch_size = min(batch_size, len(addresses))
    
    # æ‰¹é‡æŸ¥è¯¢
    try:
        # å‡†å¤‡æ‰¹é‡è¯·æ±‚
        batch_request = []
        for i, addr in enumerate(addresses[:batch_size]):
            batch_request.append({
                "jsonrpc": "2.0",
                "method": "eth_getBalance",
                "params": [Web3.to_checksum_address(addr["address"]), "latest"],
                "id": i + 1
            })
        
        # å‘é€æ‰¹é‡è¯·æ±‚
        async with session.post(
            rpc_url,
            json=batch_request,
            headers={"Content-Type": "application/json"},
            timeout=30
        ) as response:
            # è§£æå“åº”
            if response.status == 200:
                results = {}
                response_data = await response.json()
                
                # å¦‚æœå“åº”ä¸æ˜¯åˆ—è¡¨ï¼Œå¯èƒ½æ˜¯èŠ‚ç‚¹ä¸æ”¯æŒæ‰¹é‡ä½†è¿”å›äº†å•ä¸ªç»“æœ
                if not isinstance(response_data, list):
                    response_data = [response_data]
                
                for item in response_data:
                    if "result" in item and "id" in item:
                        addr_index = item["id"] - 1
                        if addr_index < len(addresses):
                            addr_info = addresses[addr_index]
                            balance_hex = item["result"]
                            try:
                                balance = int(balance_hex, 16)
                                results[addr_index] = balance
                            except:
                                results[addr_index] = 0
                
                # æ ‡è®°èŠ‚ç‚¹æˆåŠŸ
                rpc_pool.mark_node_success(rpc_url)
                
                # è¿”å›ç»“æœå’Œæœªèƒ½å¤„ç†çš„åœ°å€
                processed_addresses = addresses[:batch_size]
                remaining_addresses = addresses[batch_size:]
                return results, remaining_addresses
            else:
                # æ ‡è®°èŠ‚ç‚¹å¤±è´¥
                rpc_pool.mark_node_fail(rpc_url)
                print(f"{red}æ‰¹æ¬¡ {batch_id} RPCèŠ‚ç‚¹ {rpc_url} è¿”å›çŠ¶æ€ç : {response.status}{reset}")
                return None, addresses
    except Exception as e:
        # æ ‡è®°èŠ‚ç‚¹å¤±è´¥
        rpc_pool.mark_node_fail(rpc_url)
        print(f"{red}æ‰¹æ¬¡ {batch_id} RPCèŠ‚ç‚¹ {rpc_url} æ‰¹é‡æŸ¥è¯¢é”™è¯¯: {e}{reset}")
        return None, addresses

def generate_eth_address_from_mnemonic(mnemonic):
    """ä»åŠ©è®°è¯ç”Ÿæˆä»¥å¤ªåŠåœ°å€"""
    account_path = "m/44'/60'/0'/0/0"
    
    mnemo = Mnemonic("english")
    if not mnemo.check(mnemonic):
        raise ValueError(f"Invalid mnemonic: {mnemonic}")
    
    acct = Account.from_mnemonic(mnemonic, account_path=account_path)
    private_key = acct.key
    eth_address = acct.address
    return eth_address, private_key

# åœ°å€ç”Ÿæˆå·¥ä½œçº¿ç¨‹
def generator_worker(thread_id, bip39):
    """ç”Ÿæˆæœ‰æ•ˆåŠ©è®°è¯å¹¶åŠ å…¥é˜Ÿåˆ—"""
    global z
    queue_full_reported = False  # æ·»åŠ è¿™è¡Œåˆå§‹åŒ–å˜é‡

    
    while not stop_event.is_set():
        # æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦å·²æ»¡
        if address_queue.full():
            # åªåœ¨é¦–æ¬¡å‘ç°é˜Ÿåˆ—æ»¡æ—¶æ‰“å°æ¶ˆæ¯
            if not queue_full_reported:
                with print_lock:
                    print(f"{yellow}ç”Ÿæˆçº¿ç¨‹ {thread_id}: æŸ¥è¯¢é˜Ÿåˆ—å·²æ»¡ ({args.queue_size}/{args.queue_size})ï¼Œæš‚åœç”Ÿæˆæ–°åœ°å€{reset}")
                queue_full_reported = True
            
            # é˜Ÿåˆ—æ»¡æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´ï¼Œå‡å°‘CPUå ç”¨
            time.sleep(1.0)
            continue
        
        # é˜Ÿåˆ—ä¸å†æ»¡ï¼Œé‡ç½®æŠ¥å‘Šæ ‡å¿—
        if queue_full_reported:
            with print_lock:
                print(f"{green}ç”Ÿæˆçº¿ç¨‹ {thread_id}: æŸ¥è¯¢é˜Ÿåˆ—æœ‰ç©ºé—´ï¼Œæ¢å¤ç”Ÿæˆæ–°åœ°å€{reset}")
            queue_full_reported = False
        
        # åŸå­æ“ä½œå¢åŠ è®¡æ•°å™¨
        with counter_lock:
            z += 1
            current_z = z
            
            # æ¯1000æ¬¡è¿­ä»£ä¿å­˜ä¸€æ¬¡è¿›åº¦
            if current_z % 1000 == 0:
                with open(progress_file, "w") as f:
                    f.write(f"{current_z},{ff}")
            
            # æ¯10000æ¬¡è¿­ä»£æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ä¿¡æ¯
            if current_z % 10000 == 0:
                with print_lock:
                    print(f"{yellow}è¿›åº¦ä¿å­˜: å·²å¤„ç† {current_z} ä¸ªåŠ©è®°è¯, æ‰¾åˆ° {ff} ä¸ªæœ‰æ•ˆé’±åŒ…{reset}")
        
        
        rand_num = random.choice([12, 15, 18, 21, 24])
        mnemonic = " ".join(random.choice(bip39) for _ in range(rand_num))
        
        # ä½¿ç”¨å“ˆå¸Œå‡½æ•°ç¡®å®šè¿™ä¸ªåŠ©è®°è¯æ˜¯å¦å±äºè¯¥åˆ†ç‰‡å¤„ç†èŒƒå›´
        mnemonic_hash = int(hashlib.sha256(mnemonic.encode()).hexdigest(), 16)
        if mnemonic_hash % total_shards != shard_id:
            continue  # è·³è¿‡ä¸å±äºè¯¥åˆ†ç‰‡çš„åŠ©è®°è¯
        
        try:
            eth_addr, private_key = generate_eth_address_from_mnemonic(mnemonic)
            
            # å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—
            address_info = {
                "address": eth_addr,
                "private_key": private_key,
                "mnemonic": mnemonic,
                "count": current_z,
                "thread_id": thread_id  # æ·»åŠ çº¿ç¨‹IDä¿¡æ¯
            }
            
            # å°è¯•æ”¾å…¥é˜Ÿåˆ—ï¼Œè¶…æ—¶åç»§ç»­ç”Ÿæˆæ–°åœ°å€
            try:
                address_queue.put(address_info, timeout=1)
            except queue.Full:
                continue
                
        except ValueError:
            continue
        except Exception as e:
            with print_lock:
                print(f"{red}ç”Ÿæˆçº¿ç¨‹ {thread_id} å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}{reset}")
            continue
    
    with print_lock:
        print(f"{yellow}ç”Ÿæˆçº¿ç¨‹ {thread_id} å·²åœæ­¢{reset}")

# å¤„ç†æŸ¥è¯¢ç»“æœ
def process_balance_results(results, addresses):
    """å¤„ç†æŸ¥è¯¢è¿”å›çš„ä½™é¢ç»“æœ"""
    global ff
    
    for idx, balance in results.items():
        address_info = addresses[idx]
        eth_balance = balance / 10**18
        
        with print_lock:
            addr_space = " " * (44 - len(address_info["address"]))
            print(f"çº¿ç¨‹[{address_info.get('thread_id', 'async')}] ({address_info['count']}) åˆ†ç‰‡[{shard_id+1}/{total_shards}] ETH: {cyan}{address_info['address']}{reset}{addr_space}[Balance: {cyan}{eth_balance}{reset}]")
            print(f"Mnemonic: {yellow}{address_info['mnemonic']}{reset}")
            print(f"Private Key: {address_info['private_key'].hex()}")
            print(f"{'-' * 66}")
        
        # å¦‚æœå‘ç°æœ‰ä½™é¢ï¼Œä¿å­˜ç»“æœ
        if eth_balance > 0:
            with counter_lock:
                ff += 1
            
            with open(result_file, "a", encoding="utf-8") as dr:
                dr.write(f"ETH: {address_info['address']} | Balance: {eth_balance}\n"
                         f"Mnemonic: {address_info['mnemonic']}\n"
                         f"Private Key: {address_info['private_key'].hex()}\n\n")
            
            # åŒæ—¶ä¿å­˜åˆ°æ±‡æ€»æ–‡ä»¶
            with open("88.txt", "a", encoding="utf-8") as dr:
                dr.write(f"ETH: {address_info['address']} | Balance: {eth_balance}\n"
                         f"Mnemonic: {address_info['mnemonic']}\n"
                         f"Private Key: {address_info['private_key'].hex()}\n"
                         f"Found by Shard: {shard_id}, Thread: {address_info.get('thread_id', 'async')}\n\n")
            
            with print_lock:
                print(f"\n{green}ğŸš¨ æ‰¾åˆ°æœ‰ä½™é¢çš„é’±åŒ…! è¯¦ç»†ä¿¡æ¯å·²ä¿å­˜åˆ°{result_file}å’Œ88.txtæ–‡ä»¶ ğŸš¨{reset}\n")
            
            # ç«‹å³ä¿å­˜è¿›åº¦
            with open(progress_file, "w") as f:
                f.write(f"{z},{ff}")


# å¼‚æ­¥æŸ¥è¯¢ä¸»å¾ªç¯
async def query_main_loop():
    """å¼‚æ­¥æŸ¥è¯¢ä¸»å¾ªç¯ - ä¸é˜»å¡åœ°æ‰¹é‡æŸ¥è¯¢åœ°å€ä½™é¢"""
    # åˆ›å»ºå¼‚æ­¥HTTPä¼šè¯
    async with aiohttp.ClientSession() as session:
        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—å’Œæœªå®Œæˆä»»åŠ¡é›†åˆ
        tasks = set()
        batch_id = 0
        
        while not stop_event.is_set():
            # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæˆçš„ä»»åŠ¡
            done_tasks = {t for t in tasks if t.done()}
            tasks.difference_update(done_tasks)
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡ç»“æœ
            for task in done_tasks:
                try:
                    results, remaining_addresses = task.result()
                    if results:
                        process_balance_results(results, task.addresses)
                    
                    # å¦‚æœæœ‰æœªå¤„ç†çš„åœ°å€ï¼Œé‡æ–°åŠ å…¥é˜Ÿåˆ—
                    for addr_info in remaining_addresses:
                        try:
                            address_queue.put(addr_info, timeout=1)
                        except queue.Full:
                            pass  # é˜Ÿåˆ—æ»¡äº†ï¼Œä¸¢å¼ƒ
                except Exception as e:
                    print(f"{red}å¤„ç†æŸ¥è¯¢ç»“æœæ—¶å‡ºé”™: {e}{reset}")
            
            # æ£€æŸ¥å½“å‰å¹¶å‘ä»»åŠ¡æ•°é‡ï¼Œå¦‚æœå°‘äºè®¾å®šå€¼åˆ™æ·»åŠ æ–°ä»»åŠ¡
            while len(tasks) < args.query_concurrency and not stop_event.is_set():
                # ä»é˜Ÿåˆ—è·å–ä¸€æ‰¹åœ°å€
                addresses = []
                try:
                    # å°è¯•è·å–æ‰¹é‡åœ°å€
                    for _ in range(args.batch_size):
                        try:
                            address_info = address_queue.get_nowait()
                            addresses.append(address_info)
                            address_queue.task_done()
                        except queue.Empty:
                            break
                except Exception as e:
                    print(f"{red}è·å–åœ°å€æ—¶å‡ºé”™: {e}{reset}")
                
                # å¦‚æœè·å–åˆ°åœ°å€ï¼Œåˆ›å»ºæ–°çš„æŸ¥è¯¢ä»»åŠ¡
                if addresses:
                    batch_id += 1
                    task = asyncio.create_task(
                        async_batch_check_balances(session, addresses, batch_id)
                    )
                    task.addresses = addresses  # ä¿å­˜åœ°å€ä¿¡æ¯ä»¥ä¾¿åç»­å¤„ç†
                    tasks.add(task)
                else:
                    # é˜Ÿåˆ—ä¸ºç©ºï¼Œç­‰å¾…ç‰‡åˆ»
                    await asyncio.sleep(0.1)
                    break
            
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´åç»§ç»­å¾ªç¯
            await asyncio.sleep(0.1)
            
            # æ¯30ç§’æŠ¥å‘ŠçŠ¶æ€
            if time.time() % 30 < 0.1 and not stop_event.is_set():
                with print_lock:
                    print(f"{cyan}çŠ¶æ€: é˜Ÿåˆ— {address_queue.qsize()}/{args.queue_size}, æ´»è·ƒæŸ¥è¯¢ä»»åŠ¡ {len(tasks)}{reset}")

# è¿è¡Œå¼‚æ­¥æŸ¥è¯¢å¾ªç¯çš„çº¿ç¨‹å‡½æ•°
def run_query_loop():
    """åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥æŸ¥è¯¢å¾ªç¯"""
    global query_loop
    
    # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    query_loop = loop
    
    try:
        # è¿è¡ŒæŸ¥è¯¢ä¸»å¾ªç¯
        loop.run_until_complete(query_main_loop())
    except Exception as e:
        print(f"{red}æŸ¥è¯¢å¾ªç¯å‘ç”Ÿé”™è¯¯: {e}{reset}")
    finally:
        # å…³é—­å¾ªç¯
        try:
            loop.close()
        except:
            pass

# ä¸»ç¨‹åº
def main():
    global z, ff
    
    print(f"{cyan}å¼‚æ­¥æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–ç‰ˆ - è¿è¡Œäºåˆ†ç‰‡ {shard_id+1}/{total_shards}{reset}")
    print(f"{cyan}æŒ‰ Ctrl+C å¯ä»¥éšæ—¶åœæ­¢ç¨‹åºå¹¶ä¿å­˜è¿›åº¦{reset}")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    if not os.path.exists("bip39.txt"):
        print(f"{red}æœªæ‰¾åˆ°bip39.txtæ–‡ä»¶{reset}")
        sys.exit(1)
    
    if not os.path.exists(result_file):
        try:
            with open(result_file, "w", encoding="utf-8") as dr:
                dr.write("")
            print(f"{green}æˆåŠŸåˆ›å»º {result_file} æ–‡ä»¶{reset}")
        except Exception as e:
            print(f"{red}æ— æ³•åˆ›å»º {result_file} æ–‡ä»¶: {e}{reset}")
    
    # ä»è¿›åº¦æ–‡ä»¶æ¢å¤æˆ–åˆå§‹åŒ–è®¡æ•°
    if args.resume and os.path.exists(progress_file):
        try:
            with open(progress_file, "r") as f:
                saved_data = f.read().strip().split(",")
                z = int(saved_data[0])
                if len(saved_data) > 1:
                    ff = int(saved_data[1])
            print(f"{green}ä»è®¡æ•° {z} æ¢å¤æœç´¢è¿›åº¦ï¼Œå·²æ‰¾åˆ° {ff} ä¸ªæœ‰æ•ˆé’±åŒ…{reset}")
        except Exception as e:
            print(f"{red}æ— æ³•æ¢å¤è¿›åº¦: {e}ï¼Œå°†ä»å¤´å¼€å§‹{reset}")
            z = 0
            ff = 0

    # è¯»å–BIP39è¯å…¸
    try:
        with open("bip39.txt", "r", encoding="utf-8") as b_read:
            bip39 = [line.strip() for line in b_read.readlines() if line.strip()]
        print(f"{green}å·²åŠ è½½ {len(bip39)} ä¸ªBIP39å•è¯{reset}")
    except Exception as e:
        print(f"{red}Failed to read bip39.txt file: {e}{reset}")
        sys.exit(1)

    print(f"{yellow}åˆ†ç‰‡ä¿¡æ¯: å½“å‰åˆ†ç‰‡ {shard_id+1}/{total_shards}{reset}")
    print(f"{yellow}è¿›åº¦æ–‡ä»¶: {progress_file}{reset}")
    print(f"{yellow}ç»“æœæ–‡ä»¶: {result_file}{reset}")
    print(f"{yellow}ç”Ÿæˆçº¿ç¨‹: {args.gen_threads}, æŸ¥è¯¢å¹¶å‘: {args.query_concurrency}{reset}")
    print(f"{yellow}é˜Ÿåˆ—å¤§å°: {args.queue_size}, æ‰¹é‡å¤§å°: {args.batch_size}{reset}")
    print(f"{'-' * 66}")

    # å¯åŠ¨æŸ¥è¯¢å¾ªç¯çº¿ç¨‹
    query_thread = threading.Thread(target=run_query_loop, daemon=True)
    query_thread.start()
    print(f"{green}å¯åŠ¨å¼‚æ­¥æŸ¥è¯¢å¾ªç¯{reset}")
    
    try:
        # å¯åŠ¨ç”Ÿæˆçº¿ç¨‹
        gen_threads = []
        for i in range(args.gen_threads):
            thread = threading.Thread(
                target=generator_worker, 
                args=(i, bip39),
                daemon=True
            )
            gen_threads.append(thread)
            thread.start()
            print(f"{green}å¯åŠ¨ç”Ÿæˆçº¿ç¨‹ {i+1}{reset}")
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆæˆ–æ”¶åˆ°åœæ­¢ä¿¡å·
        while any(t.is_alive() for t in gen_threads) and query_thread.is_alive():
            if stop_event.is_set():
                break
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´
            time.sleep(1)
            
    except KeyboardInterrupt:
        # è®¾ç½®åœæ­¢äº‹ä»¶
        stop_event.set()
        print(f"\n{yellow}æ”¶åˆ°é”®ç›˜ä¸­æ–­ï¼Œæ­£åœ¨åœæ­¢çº¿ç¨‹...{reset}")
    finally:
        # ç¡®ä¿åœæ­¢äº‹ä»¶è¢«è®¾ç½®
        stop_event.set()
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        for thread in gen_threads:
            thread.join(timeout=5)
        
        query_thread.join(timeout=5)
        
        # ä¿å­˜æœ€ç»ˆè¿›åº¦
        with open(progress_file, "w") as f:
            f.write(f"{z},{ff}")
        
        print(f"\n{green}è¿›åº¦å·²ä¿å­˜: å·²å¤„ç† {z} ä¸ªåŠ©è®°è¯, æ‰¾åˆ° {ff} ä¸ªæœ‰æ•ˆé’±åŒ…{reset}")
        print(f"{green}å¯ä½¿ç”¨ --resume å‚æ•°ä»å½“å‰è¿›åº¦ç»§ç»­{reset}")

if __name__ == "__main__":
    main()