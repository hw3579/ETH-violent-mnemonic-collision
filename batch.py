import random
import requests
import time
import os
import sys
import argparse
import hashlib
import json
from mnemonic import Mnemonic
from web3 import Web3
from eth_account import Account
from colorthon import Colors
from web3.exceptions import TransactionNotFound
import concurrent.futures
import threading
import queue
import signal

# æ·»åŠ å‘½ä»¤è¡Œå‚æ•°è§£æ
parser = argparse.ArgumentParser(description='ETH Mnemonic Collision Finder - æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–ç‰ˆ')
parser.add_argument('--shard', type=int, default=0, help='å½“å‰æœåŠ¡å™¨åˆ†ç‰‡ID (0-based)')
parser.add_argument('--total-shards', type=int, default=1, help='æ€»åˆ†ç‰‡æ•°é‡')
parser.add_argument('--gen-threads', type=int, default=4, help='ç”ŸæˆåŠ©è®°è¯çš„çº¿ç¨‹æ•°é‡')
parser.add_argument('--query-threads', type=int, default=4, help='æŸ¥è¯¢çº¿ç¨‹æ•°é‡')
parser.add_argument('--resume', action='store_true', help='æ˜¯å¦ä»ä¸Šæ¬¡ä¿å­˜çš„è¿›åº¦ç»§ç»­')
parser.add_argument('--queue-size', type=int, default=10000, help='å¾…æŸ¥è¯¢é˜Ÿåˆ—å¤§å°')
args = parser.parse_args()

Account.enable_unaudited_hdwallet_features()

# å…¨å±€è®¡æ•°å™¨ï¼Œä½¿ç”¨é”ä¿æŠ¤å¹¶å‘è®¿é—®
counter_lock = threading.Lock()
z = 0  # æ€»å°è¯•æ¬¡æ•°
ff = 0  # æ‰¾åˆ°çš„æœ‰ä½™é¢é’±åŒ…æ•°

# æ‰“å°é”ï¼Œé¿å…è¾“å‡ºæ··ä¹±
print_lock = threading.Lock()

# æ·»åŠ åœæ­¢äº‹ä»¶ï¼Œç”¨äºé€šçŸ¥çº¿ç¨‹é€€å‡º
stop_event = threading.Event()

# é¢œè‰²å®šä¹‰
red = Colors.RED
green = Colors.GREEN
cyan = Colors.CYAN
yellow = Colors.YELLOW
reset = Colors.RESET

# åˆ›å»ºåˆ†ç‰‡è¿›åº¦ç›®å½•
progress_dir = "progress"
if not os.path.exists(progress_dir):
    os.makedirs(progress_dir)

# åˆ†ç‰‡ç›¸å…³å˜é‡
shard_id = args.shard
total_shards = args.total_shards
progress_file = f"{progress_dir}/progress_shard_{shard_id}.txt"

# æ¯ä¸ªåˆ†ç‰‡ä½¿ç”¨ç‹¬ç«‹çš„ç»“æœæ–‡ä»¶
result_file = f"88_shard_{shard_id}.txt"

# å®šä¹‰ä¿¡å·å¤„ç†å‡½æ•°
def handle_interrupt(signum, frame):
    print(f"\n{yellow}æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢æ‰€æœ‰çº¿ç¨‹...{reset}")
    stop_event.set()  # è®¾ç½®åœæ­¢äº‹ä»¶é€šçŸ¥æ‰€æœ‰çº¿ç¨‹

# æ³¨å†Œä¿¡å·å¤„ç†å‡½æ•°
signal.signal(signal.SIGINT, handle_interrupt)

# åœ°å€ç”Ÿæˆå’ŒæŸ¥è¯¢é˜Ÿåˆ—
address_queue = queue.Queue(maxsize=args.queue_size)

# RPCèŠ‚ç‚¹ç®¡ç†ç±» (å¢å¼ºç‰ˆæ”¯æŒæ‰¹é‡æŸ¥è¯¢)
class RPCNodePool:
    def __init__(self, rpc_file, batch_info_file=None):
        self.nodes = []
        self.node_status = {}  # è®°å½•èŠ‚ç‚¹çŠ¶æ€å’Œå¤±è´¥æ¬¡æ•°
        self.batch_info = {}   # è®°å½•èŠ‚ç‚¹æ‰¹é‡æŸ¥è¯¢èƒ½åŠ›
        self.load_nodes(rpc_file)
        self.load_batch_info(batch_info_file)
        self.pool_lock = threading.Lock()
    
    def load_nodes(self, rpc_file):
        try:
            with open(rpc_file, "r", encoding="utf-8") as file:
                self.nodes = [url.strip() for url in file.readlines() if url.strip()]
            
            for node in self.nodes:
                self.node_status[node] = {
                    "fails": 0,
                    "last_used": 0,
                    "available": True
                }
                
            print(f"{green}å·²åŠ è½½ {len(self.nodes)} ä¸ªRPCèŠ‚ç‚¹{reset}")
        except Exception as e:
            print(f"{red}æ— æ³•åŠ è½½RPCèŠ‚ç‚¹: {e}{reset}")
            sys.exit(1)
    
    def load_batch_info(self, batch_info_file):
        # é»˜è®¤å€¼ - æ‰€æœ‰èŠ‚ç‚¹é»˜è®¤æ”¯æŒå•æ¬¡æŸ¥è¯¢
        for node in self.nodes:
            self.batch_info[node] = {
                "batch_support": False,
                "max_batch_size": 1,
                "speedup": 1.0
            }
        
        # å¦‚æœæä¾›äº†æ‰¹é‡ä¿¡æ¯æ–‡ä»¶ï¼Œä»ä¸­åŠ è½½
        if batch_info_file and os.path.exists(batch_info_file):
            try:
                with open(batch_info_file, "r") as f:
                    data = json.load(f)
                
                for item in data:
                    url = item.get("url")
                    if url in self.nodes:
                        self.batch_info[url] = {
                            "batch_support": item.get("batch_request", False),
                            "max_batch_size": item.get("max_batch_size", 1) if item.get("batch_request", False) else 1,
                            "speedup": item.get("speedup", 1.0) if item.get("batch_request", False) else 1.0
                        }
                
                print(f"{green}å·²åŠ è½½ {len(data)} ä¸ªèŠ‚ç‚¹çš„æ‰¹é‡æŸ¥è¯¢ä¿¡æ¯{reset}")
                
                # æ‰“å°æ”¯æŒæ‰¹é‡æŸ¥è¯¢çš„èŠ‚ç‚¹ä¿¡æ¯
                batch_nodes = [node for node in self.nodes if self.batch_info[node]["batch_support"]]
                print(f"{green}æ”¯æŒæ‰¹é‡æŸ¥è¯¢çš„èŠ‚ç‚¹: {len(batch_nodes)}/{len(self.nodes)}{reset}")
                for node in batch_nodes:
                    max_size = self.batch_info[node]["max_batch_size"]
                    speedup = self.batch_info[node]["speedup"]
                    print(f"{cyan}èŠ‚ç‚¹ {node} æ”¯æŒæœ€å¤§æ‰¹é‡ {max_size}, åŠ é€Ÿæ¯” {speedup:.2f}x{reset}")
                    
            except Exception as e:
                print(f"{yellow}åŠ è½½æ‰¹é‡æŸ¥è¯¢ä¿¡æ¯å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤è®¾ç½®{reset}")
    
    def get_best_batch_node(self):
        """è·å–å½“å‰æœ€ä½³å¯ç”¨çš„æ‰¹é‡æŸ¥è¯¢èŠ‚ç‚¹"""
        with self.pool_lock:
            # é¦–å…ˆå°è¯•æ‰¾åˆ°æ”¯æŒæ‰¹é‡æŸ¥è¯¢çš„å¯ç”¨èŠ‚ç‚¹
            batch_nodes = [(node, info) for node, info in self.batch_info.items() 
                           if info["batch_support"] and self.node_status[node]["available"]]
            
            # æŒ‰æ‰¹é‡å¤§å°å’ŒåŠ é€Ÿæ¯”æ’åº
            batch_nodes.sort(key=lambda x: (x[1]["max_batch_size"], x[1]["speedup"]), reverse=True)
            
            # å¦‚æœæœ‰æ”¯æŒæ‰¹é‡çš„èŠ‚ç‚¹ï¼Œè¿”å›æœ€ä½³çš„
            if batch_nodes:
                node, info = batch_nodes[0]
                self.node_status[node]["last_used"] = time.time()
                return node, info["max_batch_size"]
            
            # å¦‚æœæ²¡æœ‰æ‰¹é‡èŠ‚ç‚¹å¯ç”¨ï¼Œè¿”å›ä»»æ„å¯ç”¨èŠ‚ç‚¹
            for node in self.nodes:
                if self.node_status[node]["available"]:
                    self.node_status[node]["last_used"] = time.time()
                    return node, 1
            
            # å¦‚æœæ²¡æœ‰å¯ç”¨èŠ‚ç‚¹ï¼Œé‡ç½®æ‰€æœ‰èŠ‚ç‚¹å¹¶è¿”å›ç¬¬ä¸€ä¸ª
            for node in self.nodes:
                self.node_status[node]["fails"] = 0
                self.node_status[node]["available"] = True
            
            return self.nodes[0] if self.nodes else (None, 0), 1
    
    def mark_node_fail(self, node):
        """æ ‡è®°èŠ‚ç‚¹å¤±è´¥"""
        with self.pool_lock:
            if node in self.node_status:
                self.node_status[node]["fails"] += 1
                
                # å¦‚æœè¿ç»­å¤±è´¥è¶…è¿‡é˜ˆå€¼ï¼Œæš‚æ—¶æ ‡è®°ä¸ºä¸å¯ç”¨
                if self.node_status[node]["fails"] >= 3:
                    self.node_status[node]["available"] = False
                    print(f"{yellow}RPCèŠ‚ç‚¹æš‚æ—¶ä¸å¯ç”¨: {node}{reset}")
                    
                    # 60ç§’åè‡ªåŠ¨æ¢å¤
                    threading.Timer(60, self.reset_node_status, args=[node]).start()
    
    def mark_node_success(self, node):
        """æ ‡è®°èŠ‚ç‚¹æˆåŠŸ"""
        with self.pool_lock:
            if node in self.node_status:
                self.node_status[node]["fails"] = 0
    
    def reset_node_status(self, node):
        """é‡ç½®èŠ‚ç‚¹çŠ¶æ€"""
        with self.pool_lock:
            if node in self.node_status:
                self.node_status[node]["fails"] = 0
                self.node_status[node]["available"] = True
                print(f"{green}RPCèŠ‚ç‚¹å·²æ¢å¤å¯ç”¨: {node}{reset}")

# åˆå§‹åŒ–RPCèŠ‚ç‚¹æ± ï¼Œä¼ å…¥æ‰¹é‡æŸ¥è¯¢ä¿¡æ¯
rpc_pool = RPCNodePool("rpc.txt", "rpc_test_results/all_rpc_results.json")

def batch_check_balances(addresses, thread_id):
    """æ‰¹é‡æ£€æŸ¥å¤šä¸ªåœ°å€çš„ä½™é¢"""
    rpc_url, batch_size = rpc_pool.get_best_batch_node()
    
    if not rpc_url:
        with print_lock:
            print(f"{red}çº¿ç¨‹ {thread_id} æ²¡æœ‰å¯ç”¨çš„RPCèŠ‚ç‚¹{reset}")
        return None
    
    # é™åˆ¶æ‰¹é‡å¤§å°ä¸è¶…è¿‡èŠ‚ç‚¹æ”¯æŒçš„æœ€å¤§å€¼
    batch_size = min(batch_size, len(addresses))
    
    # å¦‚æœåªæœ‰1ä¸ªåœ°å€æˆ–èŠ‚ç‚¹ä¸æ”¯æŒæ‰¹é‡ï¼Œä½¿ç”¨å•ä¸ªæŸ¥è¯¢
    if batch_size == 1:
        try:
            web3 = Web3(Web3.HTTPProvider(rpc_url, request_kwargs={'timeout': 10}))
            results = {}
            
            for addr in addresses:
                try:
                    balance = web3.eth.get_balance(addr)
                    results[addr] = balance
                except Exception as e:
                    results[addr] = 0
            
            # æ ‡è®°èŠ‚ç‚¹æˆåŠŸ
            rpc_pool.mark_node_success(rpc_url)
            return results
        except Exception as e:
            # æ ‡è®°èŠ‚ç‚¹å¤±è´¥
            rpc_pool.mark_node_fail(rpc_url)
            with print_lock:
                print(f"{red}çº¿ç¨‹ {thread_id} RPCèŠ‚ç‚¹ {rpc_url} é”™è¯¯: {e}{reset}")
            return None
    
    # æ‰¹é‡æŸ¥è¯¢
    try:
        web3 = Web3(Web3.HTTPProvider(rpc_url, request_kwargs={'timeout': 20}))
        
        # å‡†å¤‡æ‰¹é‡è¯·æ±‚
        batch_request = []
        for i, addr in enumerate(addresses):
            batch_request.append({
                "jsonrpc": "2.0",
                "method": "eth_getBalance",
                "params": [Web3.to_checksum_address(addr), "latest"],
                "id": i + 1
            })
        
        # å‘é€æ‰¹é‡è¯·æ±‚
        response = requests.post(
            rpc_url,
            json=batch_request,
            headers={"Content-Type": "application/json"},
            timeout=20
        )
        
        # è§£æå“åº”
        if response.status_code == 200:
            results = {}
            response_data = response.json()
            
            # å¦‚æœå“åº”ä¸æ˜¯åˆ—è¡¨ï¼Œå¯èƒ½æ˜¯èŠ‚ç‚¹ä¸æ”¯æŒæ‰¹é‡ä½†è¿”å›äº†å•ä¸ªç»“æœ
            if not isinstance(response_data, list):
                response_data = [response_data]
            
            for item in response_data:
                if "result" in item and "id" in item:
                    addr_index = item["id"] - 1
                    if addr_index < len(addresses):
                        addr = addresses[addr_index]
                        balance_hex = item["result"]
                        try:
                            balance = int(balance_hex, 16)
                            results[addr] = balance
                        except:
                            results[addr] = 0
            
            # æ ‡è®°èŠ‚ç‚¹æˆåŠŸ
            rpc_pool.mark_node_success(rpc_url)
            
            # å¯¹äºæœªåœ¨ç»“æœä¸­çš„åœ°å€ï¼Œè®¾ç½®ä½™é¢ä¸º0
            for addr in addresses:
                if addr not in results:
                    results[addr] = 0
                    
            return results
        else:
            # æ ‡è®°èŠ‚ç‚¹å¤±è´¥
            rpc_pool.mark_node_fail(rpc_url)
            with print_lock:
                print(f"{red}çº¿ç¨‹ {thread_id} RPCèŠ‚ç‚¹ {rpc_url} è¿”å›çŠ¶æ€ç : {response.status_code}{reset}")
            return None
    except Exception as e:
        # æ ‡è®°èŠ‚ç‚¹å¤±è´¥
        rpc_pool.mark_node_fail(rpc_url)
        with print_lock:
            print(f"{red}çº¿ç¨‹ {thread_id} RPCèŠ‚ç‚¹ {rpc_url} æ‰¹é‡æŸ¥è¯¢é”™è¯¯: {e}{reset}")
        return None

def generate_eth_address_from_mnemonic(mnemonic):
    account_path = "m/44'/60'/0'/0/0"
    
    mnemo = Mnemonic("english")
    if not mnemo.check(mnemonic):
        raise ValueError(f"Invalid mnemonic: {mnemonic}")
    
    acct = Account.from_mnemonic(mnemonic, account_path=account_path)
    private_key = acct.key
    eth_address = acct.address
    return eth_address, private_key

# åœ°å€ç”Ÿæˆå·¥ä½œçº¿ç¨‹
def generator_worker(thread_id, bip39):
    global z
    
    while not stop_event.is_set():
        # æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦å·²æ»¡
        if address_queue.full():
            time.sleep(0.1)  # é˜Ÿåˆ—æ»¡äº†ï¼Œç­‰å¾…æŸ¥è¯¢çº¿ç¨‹æ¶ˆè´¹
            continue
            
        # åŸå­æ“ä½œå¢åŠ è®¡æ•°å™¨
        with counter_lock:
            z += 1
            current_z = z
            
            # æ¯1000æ¬¡è¿­ä»£ä¿å­˜ä¸€æ¬¡è¿›åº¦
            if current_z % 1000 == 0:
                with open(progress_file, "w") as f:
                    f.write(f"{current_z},{ff}")
            
            # æ¯10000æ¬¡è¿­ä»£æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ä¿¡æ¯
            if current_z % 10000 == 0:
                with print_lock:
                    print(f"{yellow}è¿›åº¦ä¿å­˜: å·²å¤„ç† {current_z} ä¸ªåŠ©è®°è¯, æ‰¾åˆ° {ff} ä¸ªæœ‰æ•ˆé’±åŒ…{reset}")
        
        rand_num = random.choice([12, 15, 18, 21, 24])
        mnemonic = " ".join(random.choice(bip39) for _ in range(rand_num))
        
        # ä½¿ç”¨å“ˆå¸Œå‡½æ•°ç¡®å®šè¿™ä¸ªåŠ©è®°è¯æ˜¯å¦å±äºè¯¥åˆ†ç‰‡å¤„ç†èŒƒå›´
        mnemonic_hash = int(hashlib.sha256(mnemonic.encode()).hexdigest(), 16)
        if mnemonic_hash % total_shards != shard_id:
            continue  # è·³è¿‡ä¸å±äºè¯¥åˆ†ç‰‡çš„åŠ©è®°è¯
        
        try:
            eth_addr, private_key = generate_eth_address_from_mnemonic(mnemonic)
            
            # å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—
            address_info = {
                "address": eth_addr,
                "private_key": private_key,
                "mnemonic": mnemonic,
                "count": current_z
            }
            
            # å°è¯•æ”¾å…¥é˜Ÿåˆ—ï¼Œè¶…æ—¶åç»§ç»­ç”Ÿæˆæ–°åœ°å€
            try:
                address_queue.put(address_info, timeout=1)
            except queue.Full:
                continue
                
        except ValueError:
            continue
        except Exception as e:
            with print_lock:
                print(f"{red}ç”Ÿæˆçº¿ç¨‹ {thread_id} å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}{reset}")
            continue
    
    with print_lock:
        print(f"{yellow}ç”Ÿæˆçº¿ç¨‹ {thread_id} å·²åœæ­¢{reset}")

# æŸ¥è¯¢å·¥ä½œçº¿ç¨‹
def query_worker(thread_id):
    global ff
    batch_size = 10  # é»˜è®¤æ‰¹é‡å¤§å°
    
    while not stop_event.is_set():
        # ä»é˜Ÿåˆ—è·å–ä¸€æ‰¹åœ°å€
        addresses = []
        address_infos = {}
        
        # å°è¯•è·å–æ‰¹é‡åœ°å€ï¼Œä½†ä¸é˜»å¡å¤ªä¹…
        try:
            # å…ˆè·å–ä¸€ä¸ª
            address_info = address_queue.get(timeout=1)
            addresses.append(address_info["address"])
            address_infos[address_info["address"]] = address_info
            address_queue.task_done()
            
            # ç„¶åå°è¯•è·å–æ›´å¤šï¼Œç›´åˆ°è¾¾åˆ°æ‰¹é‡å¤§å°æˆ–é˜Ÿåˆ—ä¸ºç©º
            for _ in range(batch_size - 1):
                try:
                    address_info = address_queue.get(block=False)
                    addresses.append(address_info["address"])
                    address_infos[address_info["address"]] = address_info
                    address_queue.task_done()
                except queue.Empty:
                    break
        except queue.Empty:
            # é˜Ÿåˆ—ä¸ºç©ºï¼Œç­‰å¾…ç‰‡åˆ»
            time.sleep(0.1)
            continue
        
        # å¦‚æœæˆåŠŸè·å–äº†åœ°å€ï¼Œè¿›è¡Œæ‰¹é‡æŸ¥è¯¢
        if addresses:
            balances = batch_check_balances(addresses, thread_id)
            
            if balances is None:
                # æŸ¥è¯¢å¤±è´¥ï¼Œå°†åœ°å€æ”¾å›é˜Ÿåˆ—é‡è¯•
                for addr in addresses:
                    try:
                        address_queue.put(address_infos[addr], timeout=1)
                    except queue.Full:
                        pass  # é˜Ÿåˆ—æ»¡äº†ï¼Œä¸¢å¼ƒ
                continue
            
            # å¤„ç†æŸ¥è¯¢ç»“æœ
            for addr, balance in balances.items():
                if addr in address_infos:
                    address_info = address_infos[addr]
                    eth_balance = balance / 10**18
                    
                    with print_lock:
                        addr_space = " " * (44 - len(addr))
                        print(f"æŸ¥è¯¢çº¿ç¨‹[{thread_id}] ({address_info['count']}) åˆ†ç‰‡[{shard_id+1}/{total_shards}] ETH: {cyan}{addr}{reset}{addr_space}[Balance: {cyan}{eth_balance}{reset}]")
                        print(f"Mnemonic: {yellow}{address_info['mnemonic']}{reset}")
                        print(f"Private Key: {address_info['private_key'].hex()}")
                        print(f"{'-' * 66}")
                    
                    # å¦‚æœå‘ç°æœ‰ä½™é¢ï¼Œä¿å­˜ç»“æœ
                    if eth_balance > 0:
                        with counter_lock:
                            ff += 1
                        
                        with open(result_file, "a", encoding="utf-8") as dr:
                            dr.write(f"ETH: {addr} | Balance: {eth_balance}\n"
                                    f"Mnemonic: {address_info['mnemonic']}\n"
                                    f"Private Key: {address_info['private_key'].hex()}\n\n")
                        
                        # åŒæ—¶ä¿å­˜åˆ°æ±‡æ€»æ–‡ä»¶
                        with open("88.txt", "a", encoding="utf-8") as dr:
                            dr.write(f"ETH: {addr} | Balance: {eth_balance}\n"
                                    f"Mnemonic: {address_info['mnemonic']}\n"
                                    f"Private Key: {address_info['private_key'].hex()}\n"
                                    f"Found by Shard: {shard_id}, Thread: {thread_id}\n\n")
                        
                        with print_lock:
                            print(f"\n{green}ğŸš¨ æ‰¾åˆ°æœ‰ä½™é¢çš„é’±åŒ…! è¯¦ç»†ä¿¡æ¯å·²ä¿å­˜åˆ°{result_file}å’Œ88.txtæ–‡ä»¶ ğŸš¨{reset}\n")
                        
                        # ç«‹å³ä¿å­˜è¿›åº¦
                        with open(progress_file, "w") as f:
                            f.write(f"{z},{ff}")
    
    with print_lock:
        print(f"{yellow}æŸ¥è¯¢çº¿ç¨‹ {thread_id} å·²åœæ­¢{reset}")

# ä¸»ç¨‹åº
def main():
    global z, ff
    
    print(f"{cyan}æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–ç‰ˆ - è¿è¡Œäºåˆ†ç‰‡ {shard_id+1}/{total_shards}{reset}")
    print(f"{cyan}æŒ‰ Ctrl+C å¯ä»¥éšæ—¶åœæ­¢ç¨‹åºå¹¶ä¿å­˜è¿›åº¦{reset}")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    if not os.path.exists("bip39.txt"):
        print(f"{red}æœªæ‰¾åˆ°bip39.txtæ–‡ä»¶{reset}")
        sys.exit(1)
    
    if not os.path.exists(result_file):
        try:
            with open(result_file, "w", encoding="utf-8") as dr:
                dr.write("")
            print(f"{green}æˆåŠŸåˆ›å»º {result_file} æ–‡ä»¶{reset}")
        except Exception as e:
            print(f"{red}æ— æ³•åˆ›å»º {result_file} æ–‡ä»¶: {e}{reset}")
    
    # ä»è¿›åº¦æ–‡ä»¶æ¢å¤æˆ–åˆå§‹åŒ–è®¡æ•°
    if args.resume and os.path.exists(progress_file):
        try:
            with open(progress_file, "r") as f:
                saved_data = f.read().strip().split(",")
                z = int(saved_data[0])
                if len(saved_data) > 1:
                    ff = int(saved_data[1])
            print(f"{green}ä»è®¡æ•° {z} æ¢å¤æœç´¢è¿›åº¦ï¼Œå·²æ‰¾åˆ° {ff} ä¸ªæœ‰æ•ˆé’±åŒ…{reset}")
        except Exception as e:
            print(f"{red}æ— æ³•æ¢å¤è¿›åº¦: {e}ï¼Œå°†ä»å¤´å¼€å§‹{reset}")
            z = 0
            ff = 0

    # è¯»å–BIP39è¯å…¸
    try:
        with open("bip39.txt", "r", encoding="utf-8") as b_read:
            bip39 = [line.strip() for line in b_read.readlines() if line.strip()]
        print(f"{green}å·²åŠ è½½ {len(bip39)} ä¸ªBIP39å•è¯{reset}")
    except Exception as e:
        print(f"{red}Failed to read bip39.txt file: {e}{reset}")
        sys.exit(1)

    print(f"{yellow}åˆ†ç‰‡ä¿¡æ¯: å½“å‰åˆ†ç‰‡ {shard_id+1}/{total_shards}{reset}")
    print(f"{yellow}è¿›åº¦æ–‡ä»¶: {progress_file}{reset}")
    print(f"{yellow}ç»“æœæ–‡ä»¶: {result_file}{reset}")
    print(f"{yellow}ç”Ÿæˆçº¿ç¨‹: {args.gen_threads}, æŸ¥è¯¢çº¿ç¨‹: {args.query_threads}{reset}")
    print(f"{yellow}é˜Ÿåˆ—å¤§å°: {args.queue_size}{reset}")
    print(f"{'-' * 66}")

    # å¯åŠ¨ç”Ÿæˆå’ŒæŸ¥è¯¢çº¿ç¨‹
    try:
        # å¯åŠ¨ç”Ÿæˆçº¿ç¨‹
        gen_threads = []
        for i in range(args.gen_threads):
            thread = threading.Thread(
                target=generator_worker, 
                args=(i, bip39),
                daemon=True
            )
            gen_threads.append(thread)
            thread.start()
            print(f"{green}å¯åŠ¨ç”Ÿæˆçº¿ç¨‹ {i+1}{reset}")
        
        # å¯åŠ¨æŸ¥è¯¢çº¿ç¨‹
        query_threads = []
        for i in range(args.query_threads):
            thread = threading.Thread(
                target=query_worker, 
                args=(i,),
                daemon=True
            )
            query_threads.append(thread)
            thread.start()
            print(f"{green}å¯åŠ¨æŸ¥è¯¢çº¿ç¨‹ {i+1}{reset}")
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆæˆ–æ”¶åˆ°åœæ­¢ä¿¡å·
        all_threads = gen_threads + query_threads
        while any(t.is_alive() for t in all_threads):
            if stop_event.is_set():
                break
            
            # æ‰“å°é˜Ÿåˆ—çŠ¶æ€
            if not stop_event.is_set() and time.time() % 60 < 1:  # å¤§çº¦æ¯åˆ†é’Ÿ
                with print_lock:
                    print(f"{cyan}é˜Ÿåˆ—çŠ¶æ€: {address_queue.qsize()}/{args.queue_size} ({address_queue.qsize()/args.queue_size*100:.1f}%){reset}")
                time.sleep(1)  # é¿å…æ‰“å°å¤šæ¬¡
            
            time.sleep(0.5)
            
    except KeyboardInterrupt:
        # è®¾ç½®åœæ­¢äº‹ä»¶
        stop_event.set()
        print(f"\n{yellow}æ”¶åˆ°é”®ç›˜ä¸­æ–­ï¼Œæ­£åœ¨åœæ­¢çº¿ç¨‹...{reset}")
    finally:
        # ç¡®ä¿åœæ­¢äº‹ä»¶è¢«è®¾ç½®
        stop_event.set()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹ç»“æŸï¼ˆæœ€å¤šç­‰å¾…5ç§’ï¼‰
        for thread in all_threads:
            thread.join(timeout=5)
        
        # ä¿å­˜æœ€ç»ˆè¿›åº¦
        with open(progress_file, "w") as f:
            f.write(f"{z},{ff}")
        
        print(f"\n{green}è¿›åº¦å·²ä¿å­˜: å·²å¤„ç† {z} ä¸ªåŠ©è®°è¯, æ‰¾åˆ° {ff} ä¸ªæœ‰æ•ˆé’±åŒ…{reset}")
        print(f"{green}å¯ä½¿ç”¨ --resume å‚æ•°ä»å½“å‰è¿›åº¦ç»§ç»­{reset}")

if __name__ == "__main__":
    main()